{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import HyperSpectrumDataSet as hsd\n",
    "import sys\n",
    "import datetime\n",
    "import copy\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Import supplementary visualization code visuals.py\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traits = [\"LMA_O\", \"Narea_O\", \"SPAD_O\", \"Nmass_O\", \"Pmass_O\", \"Vcmax\", \"Vcmax25\", \"J\", \"Photo_O\", \"Cond_O\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_paras = {}\n",
    "\n",
    "best_paras[\"LMA_O\"] = [0.01, 3, 0.8]\n",
    "best_paras[\"Narea_O\"] = [0.01, 3, 0.8]\n",
    "best_paras[\"SPAD_O\"] = [0.05, 3, 0.8] \n",
    "best_paras[\"Nmass_O\"] = [0.1, 7, 0.5]\n",
    "best_paras[\"Pmass_O\"] = [0.1, 7, 0.8] \n",
    "best_paras[\"Vcmax\"] = [0.01, 3, 0.8] \n",
    "best_paras[\"Vcmax25\"] = [0.1, 7, 0.8] \n",
    "best_paras[\"J\"] = [0.01, 3, 0.8] \n",
    "best_paras[\"Photo_O\"] = [0.1, 9, 0.5] \n",
    "best_paras[\"Cond_O\"] = [0.01, 3, 0.8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor_optimized_drop_0.2\n",
      "Trait LMA_O -  train set: 0.9770, val set: 0.7806, test set: 0.7615\n",
      "Trait Narea_O -  train set: 0.9782, val set: 0.8717, test set: 0.8192\n",
      "Trait SPAD_O -  train set: 0.9605, val set: 0.8236, test set: 0.8318\n",
      "Trait Nmass_O -  train set: 0.9961, val set: 0.6677, test set: 0.6374\n",
      "Trait Pmass_O -  train set: 0.9998, val set: 0.5036, test set: 0.4402\n",
      "Trait Vcmax -  train set: 0.9363, val set: 0.6864, test set: 0.5702\n",
      "Trait Vcmax25 -  train set: 0.9998, val set: 0.7381, test set: 0.5078\n",
      "Trait J -  train set: 0.8588, val set: 0.7832, test set: 0.7412\n",
      "Trait Photo_O -  train set: 1.0000, val set: 0.4335, test set: 0.6474\n",
      "Trait Cond_O -  train set: 0.6199, val set: 0.1723, test set: 0.4155\n",
      "Average: train 0.9326, val 0.6461, test 0.6372\n"
     ]
    }
   ],
   "source": [
    "use_best_params = True\n",
    "\n",
    "drop_percent = 0.2\n",
    "exp_desc = \"XGBRegressor_optimized_drop_\"+str(drop_percent)\n",
    "\n",
    "run_traits = all_traits\n",
    "\n",
    "train_results_r2 = {}\n",
    "val_results_r2 = {}\n",
    "test_results_r2 = {}\n",
    "\n",
    "sum_train = 0.0\n",
    "sum_val = 0.0\n",
    "sum_test = 0.0\n",
    "print(exp_desc)\n",
    "for target_trait in run_traits:\n",
    "    train_ds = hsd.HyperSpectrumDataSet(target_trait, \"train\",drop_percent=drop_percent)\n",
    "    val_ds = hsd.HyperSpectrumDataSet(target_trait, \"val\")\n",
    "    test_ds = hsd.HyperSpectrumDataSet(target_trait, \"test\")\n",
    "        \n",
    "    X_test, y_test = test_ds.data_values, test_ds.data_labels\n",
    "    X_val, y_val = val_ds.data_values, val_ds.data_labels \n",
    "    X_train, y_train = train_ds.data_values, train_ds.data_labels\n",
    "    \n",
    "    if use_best_params:\n",
    "        learning_rate = best_paras[target_trait][0]\n",
    "        max_depth = best_paras[target_trait][1]\n",
    "        colsample_bytree = best_paras[target_trait][2]\n",
    "        subsample = 0.8\n",
    "        n_estimators = 1000\n",
    "    else:\n",
    "        learning_rate = 0.1\n",
    "        max_depth = 3\n",
    "        colsample_bytree = 1\n",
    "        n_estimators = 100\n",
    "        subsample = 1\n",
    "    \n",
    "    model = XGBRegressor(n_estimators = n_estimators, subsample = subsample, learning_rate=learning_rate, \n",
    "                          max_depth=max_depth, colsample_bytree=colsample_bytree)\n",
    "    \n",
    "    eval_set = [(X_val, y_val)]\n",
    "    \n",
    "    model.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"rmse\", eval_set=eval_set, verbose=False)\n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_val)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    train_results_r2[target_trait] = r2_score(y_train, train_preds)\n",
    "    val_results_r2[target_trait] = r2_score(y_val, val_preds)\n",
    "    test_results_r2[target_trait] = r2_score(y_test, test_preds)\n",
    "    \n",
    "    sum_train += r2_score(y_train, train_preds)\n",
    "    sum_val += r2_score(y_val, val_preds)\n",
    "    sum_test += r2_score(y_test, test_preds)\n",
    "    \n",
    "    # Report \n",
    "    print(\"Trait %s -  train set: %0.4f, val set: %0.4f, test set: %0.4f\" % (target_trait, r2_score(y_train, train_preds), r2_score(y_val, val_preds), r2_score(y_test, test_preds)))\n",
    "\n",
    "print (\"Average: train %.4f, val %.4f, test %.4f\"% (sum_train/len(train_results_r2), sum_val/len(val_results_r2), sum_test/len(test_results_r2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor_unoptimized\n",
    "Trait LMA_O -  train set: 0.9721, val set: 0.7757, test set: 0.7264\n",
    "Trait Narea_O -  train set: 0.9742, val set: 0.8804, test set: 0.8147\n",
    "Trait SPAD_O -  train set: 0.9599, val set: 0.8217, test set: 0.8183\n",
    "Trait Nmass_O -  train set: 0.9404, val set: 0.6645, test set: 0.6611\n",
    "Trait Pmass_O -  train set: 0.9586, val set: 0.5775, test set: 0.4504\n",
    "Trait Vcmax -  train set: 0.9681, val set: 0.6685, test set: 0.5407\n",
    "Trait Vcmax25 -  train set: 0.9485, val set: 0.7328, test set: 0.4625\n",
    "Trait J -  train set: 0.8779, val set: 0.7772, test set: 0.7415\n",
    "Trait Photo_O -  train set: 0.9247, val set: 0.5073, test set: 0.6643\n",
    "Trait Cond_O -  train set: 0.9013, val set: 0.1967, test set: 0.5175\n",
    "Average: train 0.9425, val 0.6602, test 0.6397\n",
    "\n",
    "\n",
    "#### Optimized \n",
    "\n",
    "Trait LMA_O -  train set: 0.9767, val set: 0.7803, test set: 0.7642\n",
    "Trait Narea_O -  train set: 0.9734, val set: 0.8741, test set: 0.8169\n",
    "Trait SPAD_O -  train set: 0.9959, val set: 0.8347, test set: 0.8322\n",
    "Trait Nmass_O -  train set: 0.9923, val set: 0.6643, test set: 0.6658\n",
    "Trait Pmass_O -  train set: 0.9998, val set: 0.5241, test set: 0.4366\n",
    "Trait Vcmax -  train set: 0.9354, val set: 0.6864, test set: 0.5688\n",
    "Trait Vcmax25 -  train set: 1.0000, val set: 0.7060, test set: 0.4727\n",
    "Trait J -  train set: 0.8982, val set: 0.7880, test set: 0.7427\n",
    "Trait Photo_O -  train set: 1.0000, val set: 0.4411, test set: 0.6544\n",
    "Trait Cond_O -  train set: 0.6199, val set: 0.1710, test set: 0.4184\n",
    "Average: train 0.9392, val 0.6470, test 0.6439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate':[0.01, 0.05, 0.1],  # default=0.1\n",
    "          'n_estimators':[1000],  # default=100\n",
    "          'max_depth': [3, 5, 7, 9],\n",
    "          'subsample':[0.8],\n",
    "          'colsample_bytree':[0.3, 0.5, 0.8]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    # Make an fbeta_score scoring object using make_scorer()\n",
    "    scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "    eval_set = [(X_val, y_val)]\n",
    "    \n",
    "    # Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 30)\n",
    "    grid_r2_score = GridSearchCV(estimator=model, param_grid=params, cv = cv, n_jobs=-1, scoring=scorer, verbose=10)\n",
    "\n",
    "    # Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "    grid_r2_score.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"rmse\", eval_set=eval_set, verbose=False)\n",
    "\n",
    "    # Best parameters\n",
    "    print(\"Best parameters found: \",grid_r2_score.best_params_)\n",
    "    # Get the estimator\n",
    "    best_xgb = grid_r2_score.best_estimator_\n",
    "\n",
    "    train_preds = best_xgb.predict(X_train)\n",
    "    val_preds = best_xgb.predict(X_val)\n",
    "    \n",
    "    train_r2_score = r2_score(y_train, train_preds)\n",
    "    val_r2_score = r2_score(y_val, val_preds)\n",
    "    \n",
    "    print(\"Best params %s -  train set: %0.4f, val set: %0.4f\" % (str(grid_r2_score.best_params_), \n",
    "                                                                  train_r2_score, val_r2_score))\n",
    "\n",
    "    return (best_xgb, grid_r2_score.best_params_, train_r2_score, val_r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = {}\n",
    "val_results = {}\n",
    "test_results = {}\n",
    "final_parameters = {}\n",
    "for target_trait in run_traits:\n",
    "    train_ds = hsd.HyperSpectrumDataSet(target_trait, \"train\")\n",
    "    val_ds = hsd.HyperSpectrumDataSet(target_trait, \"val\")\n",
    "    test_ds = hsd.HyperSpectrumDataSet(target_trait, \"test\")\n",
    "\n",
    "    X_test, y_test = test_ds.data_values, test_ds.data_labels\n",
    "    X_val, y_val = val_ds.data_values, val_ds.data_labels \n",
    "    X_train, y_train = train_ds.data_values, train_ds.data_labels\n",
    "\n",
    "    model = XGBRegressor()\n",
    "\n",
    "    best_model, b_params, train_r2, val_r2 = train_model(model, params, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    final_parameters[target_trait] = b_params \n",
    "    test_preds = best_model.predict(X_test)\n",
    "\n",
    "    train_results[target_trait] = train_r2\n",
    "    val_results[target_trait] = val_r2\n",
    "    test_results[target_trait] =  r2_score(y_test, test_preds)\n",
    "    # Report \n",
    "    print(\"Trait %s - best model -  train set: %0.4f, val set: %0.4f, test set: %0.4f\" % (target_trait, train_r2, val_r2, r2_score(y_test, test_preds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print report to file\n",
    "\n",
    "#best_params[Cond_O] = \"learning_rate: 0.1, max_depth: 3, min_child_weight: 3, n_estimators: 1000}\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "script_dir = os.path.dirname(\"__file__\")\n",
    "\n",
    "result_file_name = os.path.join(script_dir, \"results\")\n",
    "result_file_name = os.path.join(result_file_name, exp_desc + \"_\" + now.strftime(\"%Y-%m-%d-%H-%M\")+\"_results.txt\")\n",
    "\n",
    "result_file = open(result_file_name,\"w\")\n",
    "result_file.write(\"-------------------------------------------------\\n\")\n",
    "result_file.write(\"best parameters\\n\")\n",
    "\n",
    "for target_trait in run_traits:\n",
    "    result_file.write(\"trait: \" + str(final_parameters[target_trait])+\"\\n\")\n",
    "result_file.write(\"-------------------------------------------------\\n\")\n",
    "\n",
    "result_file.write(\"Here are the training data set results:\\n\")\n",
    "total = 0.0\n",
    "for key, value in train_results.items():\n",
    "    result_file.write(\"trait: \" + str(key) +\", R square: \" + str(np.asscalar(value))+\"\\n\")\n",
    "    total += np.asscalar(value)\n",
    "result_file.write(\"Average R square: \" + str(total / len(train_results))+\"\\n\")\n",
    "result_file.write(\"-------------------------------------------------\\n\")\n",
    "\n",
    "result_file.write(\"Here are the validation data set results:\\n\")\n",
    "total = 0.0\n",
    "for key, value in val_results.items():\n",
    "    result_file.write(\"trait: \" + str(key) +\", R square: \" + str(np.asscalar(value))+\"\\n\")\n",
    "    total += np.asscalar(value)\n",
    "result_file.write(\"Average R square: \" + str(total / len(train_results))+\"\\n\")\n",
    "\n",
    "result_file.write(\"-------------------------------------------------\\n\")\n",
    "result_file.write(\"Here are the test data set results: \\n\")\n",
    "\n",
    "total = 0.0\n",
    "for key, value in test_results.items():\n",
    "    result_file.write(\"trait: \" + str(key) + \", R square: \" + str(np.asscalar(value))+\"\\n\")\n",
    "    total += np.asscalar(value)\n",
    "result_file.write(\"Average R square: \" + str(total / len(test_results)) + \"\\n\")\n",
    "result_file.write(\"-------------------------------------------------\\n\")\n",
    "result_file.close()\n",
    "\n",
    "result_file = open(result_file_name+\"csv\",\"w\")\n",
    "result_file.write(\"Model, Dataset, LMA_O, Narea_O, SPAD_O, Nmass_O, Parea_O, Pmass_O, Vcmax, Vcmax25, J, Photo_O, Cond_O, Average\\n\")\n",
    "\n",
    "result_file.write(exp_desc +\", train, \") \n",
    "total = 0.0\n",
    "\n",
    "for trait in all_traits:\n",
    "    if trait in train_results:\n",
    "        result_file.write(str(np.asscalar(train_results[trait]))+\", \")\n",
    "        total += np.asscalar(train_results[trait])\n",
    "    else:\n",
    "        result_file.write(\", \")\n",
    "\n",
    "result_file.write(str(total/len(train_results))+\"\\n\")\n",
    "\n",
    "result_file.write(exp_desc +\", val, \") \n",
    "total = 0.0\n",
    "\n",
    "for trait in all_traits:\n",
    "    if trait in val_results:\n",
    "        result_file.write(str(np.asscalar(val_results[trait]))+\", \")\n",
    "        total += np.asscalar(val_results[trait])\n",
    "    else:\n",
    "        result_file.write(\", \")\n",
    "\n",
    "result_file.write(str(total/len(val_results))+\"\\n\")\n",
    "\n",
    "\n",
    "result_file.write(exp_desc +\", test, \") \n",
    "total = 0.0\n",
    "\n",
    "for trait in all_traits:\n",
    "    if trait in test_results:\n",
    "        result_file.write(str(np.asscalar(test_results[trait]))+\", \")\n",
    "        total += np.asscalar(test_results[trait])\n",
    "    else:\n",
    "        result_file.write(\", \")\n",
    "\n",
    "result_file.write(str(total/len(test_results))+\"\\n\")\n",
    "\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
